{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## FIRST MINE SOME TEXT"
      ],
      "metadata": {
        "id": "Ev7vFcgm56Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "from pprint import pprint\n",
        "from bs4 import BeautifulSoup\n",
        "from IPython.display import clear_output\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import pickle\n",
        "import copy\n",
        "import tqdm\n"
      ],
      "metadata": {
        "id": "WOPdEkxM59tw"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page = requests.get(\"https://www.nytimes.com/2020/07/31/business/trump-financial-disclosure.html\")\n",
        "soup = BeautifulSoup(page.content, 'html.parser')"
      ],
      "metadata": {
        "id": "v-el4FHp-DR_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "apikey = '' #SET your nytimes apikey\n",
        "\n",
        "\n",
        "section = \"science\"\n",
        "year=2020\n",
        "for month in [1,2,3,4,5,6,7,8]:\n",
        "  print(f\"month {month}:\")\n",
        "  query_url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={apikey}\"\n",
        "\n",
        "  r = requests.get(query_url)\n",
        "\n",
        "  results=[]\n",
        "  temp=copy.deepcopy(r.json()[\"response\"][\"docs\"])\n",
        "  for i,web in  enumerate(tqdm.tqdm(r.json()[\"response\"][\"docs\"])):\n",
        "    address=web[\"web_url\"]\n",
        "    page = requests.get(address)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "    texts=soup.find_all(\"div\",class_=\"css-s99gbd StoryBodyCompanionColumn\")\n",
        "    text=\"\"\n",
        "    for item in texts:\n",
        "      text=text+(item.text)\n",
        "    temp[i][\"full_text\"]=text\n",
        "  results.extend(temp)"
      ],
      "metadata": {
        "id": "8nyxIZXj6RuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUFhbrRcAy90",
        "outputId": "0bdcc6af-3abf-4f65-85dd-9a323ba96918"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file = open(\"/content/gdrive/MyDrive/articles_2020_8.pkl\",'rb')\n",
        "# results = pickle.load(file)\n",
        "# file.close()"
      ],
      "metadata": {
        "id": "P20x5yA7BT5H"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"\"\n",
        "for i in range(len(results)):\n",
        "  text= text+ results[i][\"full_text\"]+ \"\\n\""
      ],
      "metadata": {
        "id": "IKkQwCCvBYiW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### you can replace your own text"
      ],
      "metadata": {
        "id": "KshnKgz6-_dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the word2vec"
      ],
      "metadata": {
        "id": "ryEPK4iK6zF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"temp.txt\",\"w+\") as f:\n",
        "  f.write(text)"
      ],
      "metadata": {
        "id": "HRUYxou5CYzv"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oNpGgnBG9HIl"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "vcxGcJ8RDuAc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for `vocab_size` tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in the dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence,\n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples\n",
        "    # with a positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1,\n",
        "          num_sampled=num_ns,\n",
        "          unique=True,\n",
        "          range_max=vocab_size,\n",
        "          seed=seed,\n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "metadata": {
        "id": "1_rPXSSi-_1o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "whhMelrJAGtg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text_ds = tf.data.TextLineDataset(\"temp.txt\").filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
      ],
      "metadata": {
        "id": "9BqQsPcgAG9I"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, create a custom standardization function to lowercase the text and\n",
        "# remove punctuation.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "\n",
        "# Define the vocabulary size and the number of words in a sequence.\n",
        "vocab_size = 8192\n",
        "sequence_length = 30\n",
        "\n",
        "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
        "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
        "# same length.\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "metadata": {
        "id": "8fN5duMmAL3h"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.adapt(text_ds.batch(1024))"
      ],
      "metadata": {
        "id": "4FXYwrm_AMk4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_vocab = vectorize_layer.get_vocabulary()\n",
        "print(inverse_vocab[:40])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bOoQnEkAS_J",
        "outputId": "f5050581-fd30-4068-9286-971df5f15baa"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your', 'his', 'this', 'but', 'he', 'have', 'as', 'thou', 'him', 'so', 'what', 'thy', 'will', 'no', 'by', 'all', 'king', 'we', 'shall', 'her', 'if']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
      ],
      "metadata": {
        "id": "C-za4YW5Acz5"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())"
      ],
      "metadata": {
        "id": "dCFpdghWAjXB"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer.vocabulary_size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7G8aA-2PIbT_",
        "outputId": "02a30592-7501-43a5-82fe-e93b9a705a9f"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8192"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences,\n",
        "    window_size=3,\n",
        "    num_ns=6,\n",
        "    vocab_size=vocab_size,\n",
        "    seed=SEED)\n",
        "\n",
        "targets = np.array(targets)\n",
        "contexts = np.array(contexts)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"targets.shape: {targets.shape}\")\n",
        "print(f\"contexts.shape: {contexts.shape}\")\n",
        "print(f\"labels.shape: {labels.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dUv_X_9AqZR",
        "outputId": "63548586-5f8a-4ca3-d836-4b79399ae9c7"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32777/32777 [00:23<00:00, 1379.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "targets.shape: (114704,)\n",
            "contexts.shape: (114704, 7)\n",
            "labels.shape: (114704, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024\n",
        "num_ns = 4\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOegQW4cAsaB",
        "outputId": "caf4d199-e0b4-473d-bfc7-7214cd7cf889"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 7), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 7), dtype=tf.int64, name=None))>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2Vec(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = layers.Embedding(vocab_size,\n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\")\n",
        "    self.context_embedding = layers.Embedding(vocab_size,\n",
        "                                       embedding_dim,\n",
        "                                       input_length=num_ns+1)\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
        "    # context: (batch, context)\n",
        "    if len(target.shape) == 2:\n",
        "      target = tf.squeeze(target, axis=1)\n",
        "    # target: (batch,)\n",
        "    word_emb = self.target_embedding(target)\n",
        "    # word_emb: (batch, embed)\n",
        "    context_emb = self.context_embedding(context)\n",
        "    # context_emb: (batch, context, embed)\n",
        "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
        "    # dots: (batch, context)\n",
        "    return dots"
      ],
      "metadata": {
        "id": "ny1WNQAoA1kp"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss(x_logit, y_true):\n",
        "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
      ],
      "metadata": {
        "id": "4iwZSrwABRLJ"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "RAeIbTilBUWp"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.fit(dataset, epochs=35,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUAQOaVjBV3o",
        "outputId": "611004d5-e044-42c2-ceb2-8c78ef36e5ac"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n",
            "112/112 [==============================] - 9s 67ms/step - loss: 1.9437 - accuracy: 0.1707\n",
            "Epoch 2/35\n",
            "112/112 [==============================] - 4s 34ms/step - loss: 1.8989 - accuracy: 0.4082\n",
            "Epoch 3/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 1.7998 - accuracy: 0.3851\n",
            "Epoch 4/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 1.6843 - accuracy: 0.4252\n",
            "Epoch 5/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 1.5630 - accuracy: 0.4819\n",
            "Epoch 6/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 1.4418 - accuracy: 0.5390\n",
            "Epoch 7/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 1.3254 - accuracy: 0.5938\n",
            "Epoch 8/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 1.2166 - accuracy: 0.6424\n",
            "Epoch 9/35\n",
            "112/112 [==============================] - 4s 31ms/step - loss: 1.1160 - accuracy: 0.6834\n",
            "Epoch 10/35\n",
            "112/112 [==============================] - 3s 31ms/step - loss: 1.0240 - accuracy: 0.7168\n",
            "Epoch 11/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.9403 - accuracy: 0.7450\n",
            "Epoch 12/35\n",
            "112/112 [==============================] - 3s 31ms/step - loss: 0.8648 - accuracy: 0.7691\n",
            "Epoch 13/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.7968 - accuracy: 0.7906\n",
            "Epoch 14/35\n",
            "112/112 [==============================] - 4s 31ms/step - loss: 0.7362 - accuracy: 0.8089\n",
            "Epoch 15/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.6822 - accuracy: 0.8260\n",
            "Epoch 16/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.6339 - accuracy: 0.8400\n",
            "Epoch 17/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.5912 - accuracy: 0.8529\n",
            "Epoch 18/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.5529 - accuracy: 0.8645\n",
            "Epoch 19/35\n",
            "112/112 [==============================] - 4s 31ms/step - loss: 0.5188 - accuracy: 0.8744\n",
            "Epoch 20/35\n",
            "112/112 [==============================] - 4s 31ms/step - loss: 0.4883 - accuracy: 0.8828\n",
            "Epoch 21/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.4609 - accuracy: 0.8898\n",
            "Epoch 22/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.4363 - accuracy: 0.8954\n",
            "Epoch 23/35\n",
            "112/112 [==============================] - 4s 31ms/step - loss: 0.4142 - accuracy: 0.9013\n",
            "Epoch 24/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.3942 - accuracy: 0.9058\n",
            "Epoch 25/35\n",
            "112/112 [==============================] - 4s 33ms/step - loss: 0.3760 - accuracy: 0.9101\n",
            "Epoch 26/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.3595 - accuracy: 0.9141\n",
            "Epoch 27/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.3445 - accuracy: 0.9171\n",
            "Epoch 28/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.3307 - accuracy: 0.9203\n",
            "Epoch 29/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.3182 - accuracy: 0.9228\n",
            "Epoch 30/35\n",
            "112/112 [==============================] - 4s 33ms/step - loss: 0.3066 - accuracy: 0.9250\n",
            "Epoch 31/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.2960 - accuracy: 0.9269\n",
            "Epoch 32/35\n",
            "112/112 [==============================] - 4s 33ms/step - loss: 0.2862 - accuracy: 0.9287\n",
            "Epoch 33/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.2772 - accuracy: 0.9304\n",
            "Epoch 34/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.2689 - accuracy: 0.9317\n",
            "Epoch 35/35\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.2611 - accuracy: 0.9332\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe6facf5c40>"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ],
      "metadata": {
        "id": "MqkElKQEBpGY"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(word):\n",
        "  return weights[vectorize_layer(word)[0].numpy()]"
      ],
      "metadata": {
        "id": "iP6L8PXYFpQw"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_vec=weights\n",
        "words=np.array(vectorize_layer.get_vocabulary())\n",
        "words_length=np.sqrt((words_vec**2).sum(axis=1))"
      ],
      "metadata": {
        "id": "G7gicJ0LHDwx"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_k_closest(word_vec,k=32):\n",
        "  if type(word_vec)==str:\n",
        "    word_vec=get_embedding(word_vec)\n",
        "  word_length=np.sqrt((word_vec**2).sum())\n",
        "  return words[((word_vec*words_vec).sum(axis=1)/words_length/word_length*-1).argsort()[:k]]"
      ],
      "metadata": {
        "id": "3gWgHCYcGNx1"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "man=get_embedding(\"man\")\n",
        "woman=get_embedding(\"woman\")\n",
        "king=get_embedding(\"king\")\n",
        "\n",
        "get_k_closest(man)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtkPDID0F5mG",
        "outputId": "c0aca724-72be-4b79-b11b-6ee86f4b39df"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['uncontrolld', 'warder', 'pities', 'taketh', 'doubled',\n",
              "       'unpossessd', 'wins', 'laughing', 'gyves', 'quondam', 'spoild',\n",
              "       'infer', 'mass', 'softly', 'blubbering', 'spited', 'shelter',\n",
              "       'victor', 'sorts', 'declining', 'laboured', 'undergo',\n",
              "       'unfeignedly', 'stoppd', 'untouchd', 'confessed', 'veriest',\n",
              "       'unsafe', 'unrolled', 'clerk', 'unwillingness', 'whipped'],\n",
              "      dtype='<U18')"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TEST GENISM"
      ],
      "metadata": {
        "id": "RJ8nmSvKV0pG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "# wv = api.load('word2vec-google-news-300')\n",
        "wv = api.load('glove-wiki-gigaword-100')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3MEKeFIVJsl",
        "outputId": "a367647f-35d1-497e-d525-69a12581b46b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wv.most_similar([\"king\",\"woman\"],[\"man\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXBQss9odBIQ",
        "outputId": "4b4381fe-a9ef-4e92-e192-683017ad3e4b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.7698541283607483),\n",
              " ('monarch', 0.6843380928039551),\n",
              " ('throne', 0.6755735874176025),\n",
              " ('daughter', 0.6594556570053101),\n",
              " ('princess', 0.6520534753799438),\n",
              " ('prince', 0.6517034769058228),\n",
              " ('elizabeth', 0.6464517712593079),\n",
              " ('mother', 0.6311717629432678),\n",
              " ('emperor', 0.6106470823287964),\n",
              " ('wife', 0.6098655462265015)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words2=[]\n",
        "words_vec=[]\n",
        "for word in words:\n",
        "  try:\n",
        "    words_vec.append(wv[word])\n",
        "    words2.append(word)\n",
        "  except:\n",
        "    pass\n",
        "words_vec=np.array(words_vec)\n",
        "words=np.array(words2)"
      ],
      "metadata": {
        "id": "gUEofiy1eaRn"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(word):\n",
        "  return wv[word]"
      ],
      "metadata": {
        "id": "2McI8nZhetFG"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_length=np.sqrt((words_vec**2).sum(axis=1))"
      ],
      "metadata": {
        "id": "BEDHk32CfGpQ"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_k_closest(word_vec,k=32):\n",
        "  if type(word_vec)==str:\n",
        "    word_vec=get_embedding(word_vec)\n",
        "  word_length=np.sqrt((word_vec**2).sum())\n",
        "  return words[((word_vec*words_vec).sum(axis=1)/words_length/word_length*-1).argsort()[:k]]"
      ],
      "metadata": {
        "id": "Rl3YFaX4fPP4"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "man=get_embedding(\"man\")\n",
        "woman=get_embedding(\"woman\")\n",
        "king=get_embedding(\"king\")\n",
        "\n",
        "get_k_closest(king-man+woman)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr64bwwufVHY",
        "outputId": "941afddd-ff32-4eb4-9240-858656aceeb5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['king', 'queen', 'monarch', 'throne', 'daughter', 'prince',\n",
              "       'princess', 'mother', 'elizabeth', 'father', 'wife', 'son',\n",
              "       'sister', 'widow', 'crown', 'emperor', 'cousin', 'lady',\n",
              "       'margaret', 'married', 'kingdom', 'marriage', 'brother', 'marry',\n",
              "       'birth', 'eldest', 'niece', 'death', 'husband', 'consort',\n",
              "       'nephew', 'edward'], dtype='<U15')"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    }
  ]
}