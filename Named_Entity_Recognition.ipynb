{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waaUrODfLIk6"
      },
      "outputs": [],
      "source": [
        "!pip3 install datasets\n",
        "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "from conlleval import evaluate"
      ],
      "metadata": {
        "id": "nSSZSRKcMZxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conll_data = load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "kTdmJzSHMjvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_to_file(export_file_path, data):\n",
        "    with open(export_file_path, \"w\") as f:\n",
        "        for record in data:\n",
        "            ner_tags = record[\"ner_tags\"]\n",
        "            tokens = record[\"tokens\"]\n",
        "            if len(tokens) > 0:\n",
        "                f.write(\n",
        "                    str(len(tokens))\n",
        "                    + \"\\t\"\n",
        "                    + \"\\t\".join(tokens)\n",
        "                    + \"\\t\"\n",
        "                    + \"\\t\".join(map(str, ner_tags))\n",
        "                    + \"\\n\"\n",
        "                )\n",
        "\n",
        "\n",
        "os.mkdir(\"data\")\n",
        "export_to_file(\"./data/conll_train.txt\", conll_data[\"train\"])\n",
        "export_to_file(\"./data/conll_val.txt\", conll_data[\"validation\"])"
      ],
      "metadata": {
        "id": "U-1tnz6uMsgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tag_lookup_table():\n",
        "    iob_labels = [\"B\", \"I\"]\n",
        "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
        "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
        "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
        "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
        "    return dict(zip(range(0, len(all_labels) + 1), all_labels))\n",
        "\n",
        "\n",
        "mapping = make_tag_lookup_table()\n",
        "print(mapping)"
      ],
      "metadata": {
        "id": "zAIOA5XCMy9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n",
        "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
        "\n",
        "counter = Counter(all_tokens_array)\n",
        "print(len(counter))\n",
        "\n",
        "num_tags = len(mapping)\n",
        "vocab_size = 20000\n",
        "\n",
        "# We only take (vocab_size - 2) most commons words from the training data since\n",
        "# the `StringLookup` class uses 2 additional tokens - one denoting an unknown\n",
        "# token and another one denoting a masking token\n",
        "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
        "\n",
        "# The StringLook class will convert tokens to token IDs\n",
        "lookup_layer = keras.layers.StringLookup(\n",
        "    vocabulary=vocabulary\n",
        ")"
      ],
      "metadata": {
        "id": "IosQNS8ROSBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.data.TextLineDataset(\"./data/conll_train.txt\")\n",
        "val_data = tf.data.TextLineDataset(\"./data/conll_val.txt\")"
      ],
      "metadata": {
        "id": "dg-flCg-Os9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_record_to_training_data(record):\n",
        "    record = tf.strings.split(record, sep=\"\\t\")\n",
        "    length = tf.strings.to_number(record[0], out_type=tf.int32)\n",
        "    tokens = record[1 : length + 1]\n",
        "    tags = record[length + 1 :]\n",
        "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
        "    tags += 1\n",
        "    return tokens, tags\n",
        "\n",
        "\n",
        "def lowercase_and_convert_to_ids(tokens):\n",
        "    tokens = tf.strings.lower(tokens)\n",
        "    return lookup_layer(tokens)\n",
        "\n",
        "\n",
        "# We use `padded_batch` here because each record in the dataset has a\n",
        "# different length.\n",
        "batch_size = 32\n",
        "train_dataset = (\n",
        "    train_data.map(map_record_to_training_data)\n",
        "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
        "    # .padded_batch(batch_size)\n",
        ")\n",
        "val_dataset = (\n",
        "    val_data.map(map_record_to_training_data)\n",
        "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
        "    # .padded_batch(batch_size)\n",
        ")\n"
      ],
      "metadata": {
        "id": "qa5ov__KOxkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=np.array(list(train_dataset.as_numpy_iterator()))\n",
        "val_dataset=np.array(list(val_dataset.as_numpy_iterator()))\n",
        "\n",
        "\n",
        "train_dataset = tf.keras.preprocessing.sequence.pad_sequences(train_dataset[:,0], padding=\"post\",maxlen=64),tf.keras.preprocessing.sequence.pad_sequences(\n",
        "train_dataset[:,1], padding=\"post\",maxlen=64)\n",
        "\n",
        "val_dataset = tf.keras.preprocessing.sequence.pad_sequences(val_dataset[:,0], padding=\"post\",maxlen=64),tf.keras.preprocessing.sequence.pad_sequences(\n",
        "val_dataset[:,1], padding=\"post\",maxlen=64)"
      ],
      "metadata": {
        "id": "wzOdti8KbJEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=tf.data.Dataset.from_tensor_slices(\n",
        "    train_dataset\n",
        ")\n",
        "val_dataset=tf.data.Dataset.from_tensor_slices(\n",
        "    val_dataset\n",
        ")"
      ],
      "metadata": {
        "id": "78zVmEXfeaKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NOW MODEL"
      ],
      "metadata": {
        "id": "KfBe24FQQcJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
        "    def __init__(self, name=\"custom_ner_loss\"):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=True, reduction=keras.losses.Reduction.NONE\n",
        "        )\n",
        "        loss = loss_fn(y_true, y_pred)\n",
        "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
        "        loss = loss * mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "loss = CustomNonPaddingTokenLoss()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def custom_metric(y_true, y_pred):\n",
        "  metric=tf.math.argmax(y_pred,axis=-1)==tf.cast(y_true, dtype=tf.int64)\n",
        "  mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
        "  metric = tf.cast(metric,dtype=tf.float32) * mask\n",
        "  return tf.reduce_sum(metric) / tf.reduce_sum(mask)\n"
      ],
      "metadata": {
        "id": "tb1exB4EO1jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp=tf.keras.layers.Input((64,))\n",
        "embedding=tf.keras.layers.Embedding(vocab_size - 1, 64)(inp)\n",
        "# att1=tf.keras.layers.MultiHeadAttention(num_heads=5,key_dim=64,output_shape=128)(embedding,embedding)\n",
        "# att2=tf.keras.layers.MultiHeadAttention(num_heads=5,key_dim=64,output_shape=128)(att1,att1)\n",
        "out=tf.keras.layers.Dense(num_tags, activation=\"softmax\")(embedding)\n",
        "ner_model=tf.keras.Model(inputs=inp,outputs=out)\n",
        "ner_model.summary()"
      ],
      "metadata": {
        "id": "cFLValQZfT4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class TransformerBlock(layers.Layer):\n",
        "#     def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "#         super(TransformerBlock, self).__init__()\n",
        "#         self.att = keras.layers.MultiHeadAttention(\n",
        "#             num_heads=num_heads, key_dim=embed_dim\n",
        "#         )\n",
        "#         self.ffn = keras.Sequential(\n",
        "#             [\n",
        "#                 keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "#                 keras.layers.Dense(embed_dim),\n",
        "#             ]\n",
        "#         )\n",
        "#         self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "#         self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "#         self.dropout1 = keras.layers.Dropout(rate)\n",
        "#         self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "#     def call(self, inputs, training=False):\n",
        "#         attn_output = self.att(inputs, inputs)\n",
        "#         attn_output = self.dropout1(attn_output, training=training)\n",
        "#         out1 = self.layernorm1(inputs + attn_output)\n",
        "#         ffn_output = self.ffn(out1)\n",
        "#         ffn_output = self.dropout2(ffn_output, training=training)\n",
        "#         return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "# class TokenAndPositionEmbedding(layers.Layer):\n",
        "#     def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "#         super(TokenAndPositionEmbedding, self).__init__()\n",
        "#         self.token_emb = keras.layers.Embedding(\n",
        "#             input_dim=vocab_size, output_dim=embed_dim\n",
        "#         )\n",
        "#         self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "#     def call(self, inputs):\n",
        "#         maxlen = tf.shape(inputs)[-1]\n",
        "#         positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "#         position_embeddings = self.pos_emb(positions)\n",
        "#         token_embeddings = self.token_emb(inputs)\n",
        "#         return token_embeddings + position_embeddings\n",
        "\n",
        "\n",
        "\n",
        "# class NERModel(keras.Model):\n",
        "#     def __init__(\n",
        "#         self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32\n",
        "#     ):\n",
        "#         super(NERModel, self).__init__()\n",
        "#         self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "#         self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "#         self.dropout1 = layers.Dropout(0.1)\n",
        "#         self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
        "#         self.dropout2 = layers.Dropout(0.1)\n",
        "#         self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
        "\n",
        "#     def call(self, inputs, training=False):\n",
        "#         x = self.embedding_layer(inputs)\n",
        "#         x = self.transformer_block(x)\n",
        "#         x = self.dropout1(x, training=training)\n",
        "#         x = self.ff(x)\n",
        "#         x = self.dropout2(x, training=training)\n",
        "#         x = self.ff_final(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)"
      ],
      "metadata": {
        "id": "t_GZXWCAn4Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss=loss,metrics=[custom_metric])\n",
        "ner_model.fit(train_dataset.batch(64), epochs=100,validation_data=val_dataset.batch(64))"
      ],
      "metadata": {
        "id": "oReKwAEIO55d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_convert_to_ids(text):\n",
        "    tokens = text.split()\n",
        "    return lowercase_and_convert_to_ids(tokens)\n",
        "\n",
        "\n",
        "# Sample inference using the trained model\n",
        "sample_input = tokenize_and_convert_to_ids(\n",
        "    \"Hi Hussain , do you know the Nottinghamshire\"\n",
        ")\n",
        "sample_input = tf.reshape(sample_input, shape=[1, -1])\n",
        "print(sample_input)\n",
        "\n",
        "sample_input=tf.keras.preprocessing.sequence.pad_sequences(sample_input, padding=\"post\",maxlen=64)\n",
        "\n",
        "\n",
        "\n",
        "output = ner_model.predict(sample_input)\n",
        "prediction = np.argmax(output, axis=-1)[0]\n",
        "prediction = [mapping[i] for i in prediction]\n",
        "\n",
        "# eu -> B-ORG, german -> B-MISC, british -> B-MISC\n",
        "print(prediction)"
      ],
      "metadata": {
        "id": "1wmTDNKJO72v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnt=0\n",
        "for element in val_data:\n",
        "  print(tf.strings.regex_replace(element, \"\\t\", \" \"))\n",
        "  cnt+=1\n",
        "  if cnt==10:\n",
        "    break"
      ],
      "metadata": {
        "id": "SCfn3sf1GWxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(dataset):\n",
        "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
        "\n",
        "    for x, y in dataset:\n",
        "        # print(x)\n",
        "        output = ner_model.predict(x)\n",
        "        predictions = np.argmax(output, axis=-1)\n",
        "        predictions = np.reshape(predictions, [-1])\n",
        "\n",
        "        true_tag_ids = np.reshape(y, [-1])\n",
        "\n",
        "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
        "        true_tag_ids = true_tag_ids[mask]\n",
        "        predicted_tag_ids = predictions[mask]\n",
        "\n",
        "        all_true_tag_ids.append(true_tag_ids)\n",
        "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
        "\n",
        "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
        "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
        "\n",
        "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
        "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
        "\n",
        "    evaluate(real_tags, predicted_tags)\n",
        "\n",
        "\n",
        "calculate_metrics(val_dataset.batch(64))"
      ],
      "metadata": {
        "id": "T1MZXnOBIm3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U2H6m5rrKCKL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}