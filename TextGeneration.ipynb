{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "\n",
        "import io\n",
        "import re\n",
        "import string\n",
        "import tqdm\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "Rg__6ie2R44p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DATA PROVISION"
      ],
      "metadata": {
        "id": "Fsap8YnzQG1s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRpu9ManNjYg"
      },
      "outputs": [],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Read, then decode for py2 compat.\n",
        "# text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# # length of text is the number of characters in it\n",
        "# print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "id": "0i6TlLiNR_jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Take a look at the first 250 characters in text\n",
        "# print(text[:250])"
      ],
      "metadata": {
        "id": "SSPpfEKvSDDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngrams=4\n",
        "text=tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
        "text=text.filter(lambda x:len(tf.strings.split(x,\" \"))>ngrams-1)\n",
        "# text=text.map(lambda x:tf.strings.unicode_decode(x, 'UTF-8'))"
      ],
      "metadata": {
        "id": "VdDH7Zl8SHAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list(map(lambda x: x.decode(\"utf-8\"),list(text.as_numpy_iterator())))"
      ],
      "metadata": {
        "id": "4bNLaXn1xJA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=16384\n",
        "tokenizer=tf.keras.preprocessing.text.Tokenizer(\n",
        "    num_words=vocab_size,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True,\n",
        "    split=' ',\n",
        "    char_level=False,\n",
        "    oov_token=None,\n",
        "    analyzer=None,\n",
        ")"
      ],
      "metadata": {
        "id": "xKHdcdCwpBbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts=list(map(lambda x: x.decode(\"utf-8\"),list(text.as_numpy_iterator())))"
      ],
      "metadata": {
        "id": "fjb5MGk2xALj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(texts)"
      ],
      "metadata": {
        "id": "RnsFjkWPrfjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences=tokenizer.texts_to_sequences(texts)"
      ],
      "metadata": {
        "id": "bfi30PtOtq8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_sequences=[]\n",
        "for seq in sequences:\n",
        "  l=len(seq)\n",
        "  for i in range(2,ngrams):\n",
        "    new_sequences.append(seq[:i])\n",
        "  for i in range(ngrams,l):\n",
        "    new_sequences.append(seq[i-ngrams:i])\n",
        "\n",
        "new_sequences = np.array(pad_sequences(new_sequences, maxlen=4, padding='pre'))\n",
        "new_sequences_x= new_sequences[:,:-1]\n",
        "new_sequences_y= new_sequences[:,-1]\n",
        "l=len(new_sequences)\n",
        "thr=int(l*0.7)"
      ],
      "metadata": {
        "id": "0pIgI20d0UBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_config()[\"word_index\"]"
      ],
      "metadata": {
        "id": "OYPsNj0dDACt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train=tf.data.Dataset.from_tensor_slices((new_sequences_x[:thr],new_sequences_y[:thr]))\n",
        "data_test=tf.data.Dataset.from_tensor_slices((new_sequences_x[thr:],new_sequences_y[thr:]))"
      ],
      "metadata": {
        "id": "h7cvNyZR11GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp=tf.keras.layers.Input((ngrams-1,))\n",
        "embedding=tf.keras.layers.Embedding(vocab_size - 1, 64)(inp)\n",
        "# att1=tf.keras.layers.MultiHeadAttention(num_heads=3,key_dim=32,output_shape=64)(embedding,embedding)\n",
        "# att2=tf.keras.layers.MultiHeadAttention(num_heads=3,key_dim=32,output_shape=64)(att1,att1)\n",
        "lstm1=tf.keras.layers.LSTM(128,return_sequences=True)(embedding)\n",
        "lstm2=tf.keras.layers.LSTM(128)(lstm1)\n",
        "flatten=tf.keras.layers.Flatten()(lstm2)\n",
        "out=tf.keras.layers.Dense(vocab_size-1, activation=\"softmax\")(flatten)\n",
        "model=tf.keras.Model(inputs=inp,outputs=out)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "SPc_HoCj2bTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.optimizers.RMSprop(),loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=False,\n",
        "        ),metrics=[\"accuracy\"])\n",
        "\n",
        "checkpoint_filepath = 'temp.ckpt'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n"
      ],
      "metadata": {
        "id": "uyVce9Th3Kwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(data_train.batch(64),epochs=128,validation_data=data_test.batch(64),callbacks=[model_checkpoint_callback])"
      ],
      "metadata": {
        "id": "ETAISFES3p65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(checkpoint_filepath)"
      ],
      "metadata": {
        "id": "3iVxGQwLI0Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(input_sequence):\n",
        "  seq=tokenizer.texts_to_sequences([input_sequence])[0]\n",
        "  if len(seq)>4:\n",
        "    seq=seq[-4:]\n",
        "  seq=pad_sequences([seq], maxlen=ngrams-1, padding='pre')[0]\n",
        "  seq=list(seq)\n",
        "  seq.append(model.predict(np.array([seq]))[0].argmax())\n",
        "  if len(seq)>4:\n",
        "    seq=seq[-4:]\n",
        "  seq_inv=tokenizer.sequences_to_texts([seq])[0]\n",
        "  input_sequence+=(\" \"+seq_inv.split(\" \")[-1])\n",
        "  return input_sequence"
      ],
      "metadata": {
        "id": "97AAEAsc6S9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq=\"god of old\"\n",
        "for i in range(40):\n",
        "  seq=predict_next_word(seq)\n",
        "print(seq)"
      ],
      "metadata": {
        "id": "c2EI26f9BXeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq=\"king of \"\n",
        "for i in range(40):\n",
        "  seq=predict_next_word(seq)\n",
        "print(seq)"
      ],
      "metadata": {
        "id": "mBu1o0dpH_Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cnt=0\n",
        "# for el in data:\n",
        "#   print(el)\n",
        "#   cnt+=1\n",
        "#   if cnt==10:\n",
        "#     break"
      ],
      "metadata": {
        "id": "PFemrJjMalBO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}