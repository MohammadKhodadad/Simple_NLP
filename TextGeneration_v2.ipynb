{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lSG4E5zJnyZ",
        "outputId": "b4a01a24-351f-4662-a140-4fd4cafe3bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.7/527.7 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade keras-nlp tensorflow #For Keras-NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOQkWzHudzuj"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "import math\n",
        "import io\n",
        "import re\n",
        "import string\n",
        "import tqdm\n",
        "\n",
        "import keras_nlp\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvhUpMWPRJTT"
      },
      "outputs": [],
      "source": [
        "!wget http://www.cs.biu.ac.il/~koppel/blogs/blogs.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpMc2KRaa3Gj"
      },
      "outputs": [],
      "source": [
        "!unzip blogs.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8NEpi3lcJTW"
      },
      "outputs": [],
      "source": [
        "!pip install lxml\n",
        "!pip install bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOsGnaxObu0K"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup as bs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hmWYfk1c0NX"
      },
      "outputs": [],
      "source": [
        "\n",
        "contents=[]\n",
        "for f in tqdm.tqdm(os.listdir('blogs')):\n",
        "  with open(f'blogs/{f}', 'rb') as file_:\n",
        "    lines=file_.read()\n",
        "    contents.append(bs(lines, \"lxml\").text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSRxEnlsdV55"
      },
      "outputs": [],
      "source": [
        "text='\\n'.join(contents[:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e0M3stFfeMK"
      },
      "outputs": [],
      "source": [
        "with open('temp.txt','w') as f:\n",
        "  f.write(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4t3nLiDqeBZT"
      },
      "outputs": [],
      "source": [
        "# path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-UZ-8NreD-i"
      },
      "outputs": [],
      "source": [
        "ngrams=5\n",
        "path_to_file='temp.txt',\n",
        "text=tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
        "text=text.filter(lambda x:len(tf.strings.split(x,\" \"))>ngrams-1)\n",
        "# text=text.map(lambda x:tf.strings.unicode_decode(x, 'UTF-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2-hE6yIeM2M"
      },
      "outputs": [],
      "source": [
        "vocab_size=16384\n",
        "tokenizer=tf.keras.preprocessing.text.Tokenizer(\n",
        "    num_words=vocab_size,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True,\n",
        "    split=' ',\n",
        "    char_level=False,\n",
        "    oov_token=None,\n",
        "    analyzer=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nKgyqHKeYgd"
      },
      "outputs": [],
      "source": [
        "texts=list(map(lambda x: x.decode(\"utf-8\"),list(text.as_numpy_iterator())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkCkeMABeecN"
      },
      "outputs": [],
      "source": [
        "tokenizer.fit_on_texts(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FALsRt3leu9v"
      },
      "outputs": [],
      "source": [
        "sequences=tokenizer.texts_to_sequences(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHO5ZOM-exPo"
      },
      "outputs": [],
      "source": [
        "new_sequences=[]\n",
        "for seq in sequences:\n",
        "  l=len(seq)\n",
        "  # for i in range(2,ngrams):\n",
        "  #   new_sequences.append(seq[:i])\n",
        "  for i in range(ngrams,l):\n",
        "    new_sequences.append(seq[i-ngrams:i])\n",
        "\n",
        "new_sequences = np.array(pad_sequences(new_sequences, maxlen=5, padding='pre'))\n",
        "new_sequences_x= new_sequences[:,:-1]\n",
        "new_sequences_y= new_sequences[:,1:]\n",
        "l=len(new_sequences)\n",
        "thr=int(l*0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyHHifUdfDUL"
      },
      "outputs": [],
      "source": [
        "data_train=tf.data.Dataset.from_tensor_slices((new_sequences_x[:thr],new_sequences_y[:thr]))\n",
        "data_test=tf.data.Dataset.from_tensor_slices((new_sequences_x[thr:],new_sequences_y[thr:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-mjydf6fKFp"
      },
      "outputs": [],
      "source": [
        "inp=tf.keras.layers.Input((ngrams-1,))\n",
        "embedding=tf.keras.layers.Embedding(vocab_size, 64)(inp)\n",
        "# att1=tf.keras.layers.MultiHeadAttention(num_heads=3,key_dim=32,output_shape=64)(embedding,embedding)\n",
        "# att2=tf.keras.layers.MultiHeadAttention(num_heads=3,key_dim=32,output_shape=64)(att1,att1)\n",
        "lstm1=tf.keras.layers.LSTM(128,return_sequences=True)(embedding)\n",
        "lstm2=tf.keras.layers.LSTM(128,return_sequences=True)(lstm1)\n",
        "# features1=tf.keras.layers.GlobalAveragePooling1D()(att2)\n",
        "# flatten=tf.keras.layers.Flatten()(lstm2)\n",
        "out=tf.keras.layers.Dense(vocab_size, activation=tf.keras.layers.Softmax(axis=-1))(lstm2)\n",
        "model=tf.keras.Model(inputs=inp,outputs=out)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvYAeB2Gh0RZ"
      },
      "outputs": [],
      "source": [
        "y_train=new_sequences_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xII-es37pgBa"
      },
      "outputs": [],
      "source": [
        "y_train=np.array(y_train)[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXr8Ag3uE1l2",
        "outputId": "f9362737-eba5-46e1-9e4d-b125b4052aa6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    1,     2,     3, ..., 16381, 16382, 16383], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "np.unique(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcBCOkxlpA5E",
        "outputId": "963068bd-985b-4d73-9dad-c700c9dc2820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16384/16384 [01:15<00:00, 216.26it/s]\n"
          ]
        }
      ],
      "source": [
        "class_weight={}\n",
        "for i in tqdm.tqdm(range(vocab_size)):\n",
        "  freq=(np.where(y_train==i,1,0).sum()+1)/len(y_train)\n",
        "  freq=math.pow(freq,0.66)\n",
        "  class_weight[i]=1/freq\n",
        "  if class_weight[i]>30:\n",
        "    class_weight[i]=30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsmtYdPNf9IB"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.optimizers.RMSprop(learning_rate=0.001),loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=False,\n",
        "        ),metrics=[\"accuracy\",keras_nlp.metrics.Perplexity()])\n",
        "\n",
        "checkpoint_filepath = 'temp.ckpt'\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUlBlMXHjSM5"
      },
      "outputs": [],
      "source": [
        "model.fit(data_train.batch(32),epochs=128,validation_data=data_test.batch(32),callbacks=[model_checkpoint_callback],class_weight=class_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdlllnDOFA2M"
      },
      "outputs": [],
      "source": [
        "out=model.predict(new_sequences_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Boc3MnEimFzQ"
      },
      "outputs": [],
      "source": [
        "model.load_weights(checkpoint_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxR2MmFgkVdI"
      },
      "outputs": [],
      "source": [
        "def predict_next_word(input_sequence):\n",
        "  seq=tokenizer.texts_to_sequences([input_sequence])[0]\n",
        "  if len(seq)>4:\n",
        "    seq=seq[-4:]\n",
        "    l=3\n",
        "  else:\n",
        "    l=len(seq)-1\n",
        "    seq=pad_sequences([seq], maxlen=ngrams-1, padding='post')[0]\n",
        "  seq=list(seq)\n",
        "  seq.append(model.predict(np.array([seq]))[0].argmax(axis=-1)[l])\n",
        "  if len(seq)>4:\n",
        "    seq=seq[-4:]\n",
        "  seq_inv=tokenizer.sequences_to_texts([np.array(seq)])[0]\n",
        "  input_sequence+=(\" \"+seq_inv.split(\" \")[-1])\n",
        "  return input_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApWzLMfBmM-7"
      },
      "outputs": [],
      "source": [
        "seq=\"you have seen\"\n",
        "for i in range(20):\n",
        "  seq=predict_next_word(seq)\n",
        "print(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEv7ZcfqgDXw"
      },
      "outputs": [],
      "source": [
        "(new_sequences_x>16382).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXVacuxBITrk"
      },
      "outputs": [],
      "source": [
        "new_sequences_x.shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}